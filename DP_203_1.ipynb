{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgq3jlZXDDD6DhJxWZu7as",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nenuchepanu001/DP-203/blob/main/DP_203_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Storage Account Redundancy:\n",
        "      1. Multiple Copies of data are stored\n",
        "      2. This helps to protect aganist planned and unplanned events\n",
        "        - transient hardware failures\n",
        "        - network or power outages\n",
        "        - natural disasters like earth quakes or floods\n",
        "\n",
        "      Types of Redundancy:\n",
        "\n",
        "        Locally-Redundant-Storage (LRS):\n",
        "            copied to three different storage devices.\n",
        "            This helps  to protect againist Server/Rack/Driver Failures.\n",
        "            Disadv:\n",
        "              if something happens at datacenter level then all three storage devices will not be accessable at same point of time and bcos of that your data will be lost and data cannot accessible.\n",
        "\n",
        "        Zone-Redundant-Storage (ZRS):\n",
        "            This helps to protect aganist data center level failures\n",
        "            Data replicated synchronously across three azure availability zones\n",
        "            Each availability zone is a seperate physical location with independent power, colling and networking.\n",
        "            **Note:** 3 zones will be in same region.\n",
        "            Disadv:\n",
        "              if region goes down all available zones will be go down.\n",
        "\n",
        "        Geo-Redundant-Storage (GRS):\n",
        "            Data is replicated to another Region (Azure Region Pairs)\n",
        "            This helps to protect aganist region level failures\n",
        "            like if\n",
        "                    primary region: central us\n",
        "                    secondary region: East us\n",
        "\n",
        "        Read access Geo-Redundant-Storage (RA GRS):\n",
        "              we can access secondary region only if primary is gone down in GRS\n",
        "              otherwise we cannot access secondary region\n",
        "\n",
        "              we can access RA GRS simultaneously\n",
        "\n",
        "\n",
        "https://www.udemy.com/course/azure-data-factory-masterclass-from-zero-to-hero/learn/lecture/35247856?start=285#overview\n",
        "\n",
        "    Access Tier:\n",
        "      Hot Access Tier:\n",
        "          used for data accessed frequently\n",
        "\n",
        "      Cool Access Tier:\n",
        "          used for data that is accessed infrequently and stored for atleast 30 days\n",
        "\n",
        "      Archive Access Tier:\n",
        "          used for data that is rarely used and stored for atleast 180 days\n",
        "              - you have to rehydrate the file (changing the access tier to hot or cool) to access the file\n",
        "              - rehydrate the file (standard priority - 15 hours | high priority 1 hour for file less than 10 gb)\n",
        "\n",
        "    Linked Services:\n",
        "        Linked services are much like connection strings, which define the connection information that's needed for data factory to connect to external resources\n",
        "\n",
        "**Design and implement data storage:**\n",
        "\n",
        "**Develop data processing (40-45%):**\n",
        "\n",
        "    Transform data by using Transact-SQL (T-SQL):\n",
        "\n",
        "        T-SQL runs INSERT, UPDATE and DELETE operations on a target table from a results of a join with a source table.\n",
        "\n",
        "        use merge to perform:\n",
        "            - INSERT and  UPDATE operations on a table in single statement\n",
        "            - UPDATE and DELETE operations on a table in single statement\n",
        "            - UPDATE and INSERT operations on a target table by using a derived source table\n",
        "            - INSERT or UPDATE on a target edge table in a graph database\n",
        "            - INSERT the results of the MERGE statement into another table\n",
        "        \n",
        "        some of key activities to transform data are as follows:\n",
        "            Script activity:\n",
        "                Allows user to insert, modify, delete and retrive data in the database and create,modify and remove database objects such as tables,indexes and users\n",
        "\n",
        "            Stored procedure:\n",
        "                Enables users to invoke a stored procedure in azure sql database,azure synapse analytics or SQL server database\n",
        "\n",
        "\n",
        "        Ingest and Transform data by using Azure Synapse Pipelines or ADF:\n",
        "\n",
        "            Perform data transformations with azure synapse pipelines using code free using the mapping data flow task\n",
        "\n",
        "                prepare the environment -->\n",
        "                Add a data source -->\n",
        "                use mapping data flow tranformation  -->\n",
        "                write to data sink -->\n",
        "                run the pipeline\n",
        "            \n",
        "            - Ingest data using ADF as it empowers you to do code-free Extract,Transform, Load (ETL) or Extract, Load , Transform (ELT) including preperation and tranformation.\n",
        "            - Consider a hybrid approach and combine ADF, Databricks and power bi with azure synapse analytics\n",
        "        \n",
        "\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "58edzBnCDcmK"
      }
    }
  ]
}